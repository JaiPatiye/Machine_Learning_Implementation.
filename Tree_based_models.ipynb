{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-yI_sjl02if"
   },
   "source": [
    "## Agenda\n",
    "\n",
    "- Decision Tree \n",
    "- Pruning/Tuning a Decision Tree model\n",
    "- GridSearchCV and RandomizedSearchCV\n",
    "- Random Forest\n",
    "- Tuning Random Forest\n",
    "- Gradient Boosting Methods \n",
    "- Tuning GBMs\n",
    "\n",
    "### Margin Separation\n",
    "- Support Vector Machines\n",
    "- Tuning SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH12B6zW02ik"
   },
   "source": [
    "## Problem statement:\n",
    "\n",
    "ABC Bank has provided us with a dataset that contains customer details for their customers in `BankAttrition - Details.csv` file. The transactions related information and what kind of credit card the customer holds is provided to us in another file `Transaction and Card Details.csv`. The bank is currently facing problems of customer attrition. They have consulted us to understand how can they understand the patterns of customer attrition and if they can get early signals so to stop losing customers.\n",
    "\n",
    "Till now: Merged data, performed exploratory data analysis, KNN and Logistic Regression Models, Validation Strategies, Model Improvement Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew1nvJqQ02im",
    "outputId": "cc6e8e0b-05c3-4fe7-f51b-c8d41a6d5edb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10127, 8), (10127, 14))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read input files\n",
    "details = pd.read_csv(\"Datasets/BankAttrition - Details.csv\")\n",
    "transaction = pd.read_csv(\"Datasets/Transaction and Card Details.csv\")\n",
    "\n",
    "details.shape, transaction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHO2d4b502io"
   },
   "outputs": [],
   "source": [
    "# merge to create ADS\n",
    "ads = pd.merge(details, transaction, how = 'outer', on = ['CLIENTNUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FAfjvlV02ip"
   },
   "outputs": [],
   "source": [
    "## consider Unknown as a separate category\n",
    "\n",
    "# typecasting variables\n",
    "ads['Gender'] = ads['Gender'].astype('category')\n",
    "ads['Education_Level'] = ads['Education_Level'].astype('category')\n",
    "ads['Marital_Status'] = ads['Marital_Status'].astype('category')\n",
    "ads['Income_Category'] = ads['Income_Category'].astype('category')\n",
    "ads['Card_Category'] = ads['Card_Category'].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "# encoding target to - 0, 1\n",
    "ads['Attrition_Flag'] = ads['Attrition_Flag'].map({'Existing Customer':0,'Attrited Customer':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeRfG4kZ02ip"
   },
   "outputs": [],
   "source": [
    "# drop ClientNum as it is just the identifier\n",
    "ads.drop([\"CLIENTNUM\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCQeKS8T02iq"
   },
   "outputs": [],
   "source": [
    "# One hot encoding the categories\n",
    "categorical_vars = ads.select_dtypes(exclude = ['int64', 'Int64', 'float64']).columns\n",
    "ads = pd.get_dummies(ads, columns = categorical_vars)\n",
    "\n",
    "ads['Attrition_Flag'] = ads['Attrition_Flag'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCc6lrDV02ir"
   },
   "outputs": [],
   "source": [
    "## Feature engineering - log transformation (Credit Limit, Total Revolving Balance)\n",
    "ads['Credit_Limit'] = np.log(ads['Credit_Limit'])\n",
    "ads['Total_Revolving_Bal'] = np.log(ads['Total_Revolving_Bal'] + 0.01)\n",
    "ads['Total_Trans_Amt'] = np.log(ads['Total_Trans_Amt'] + 0.01)\n",
    "\n",
    "## Feature engineering - Customer Age bins\n",
    "bins = [0, 18, 30, 50, 70, 110]\n",
    "ads['binned_age'] = pd.cut(ads['Customer_Age'], bins)\n",
    "ads = pd.get_dummies(ads, columns = ['binned_age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc-2_PTd02is",
    "outputId": "6baad905-d4c3-4be1-f6fc-7a17ad512154"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10127, 42), (10127,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seperating independent and dependent variables\n",
    "x = ads.drop(['Attrition_Flag'], axis=1)\n",
    "y = ads['Attrition_Flag']\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anw3sP_i02it"
   },
   "outputs": [],
   "source": [
    "# importing the train test split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, random_state = 111, stratify = y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPiQo_dG02iu",
    "outputId": "420df602-e29a-4a01-d9c9-4ea86af631ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    0.839368\n",
       " 1    0.160632\n",
       " Name: Attrition_Flag, dtype: float64,\n",
       " 0    0.839258\n",
       " 1    0.160742\n",
       " Name: Attrition_Flag, dtype: float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check proportions of target variables\n",
    "train_y.value_counts(normalize = True), test_y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsxyO_vr02iv"
   },
   "outputs": [],
   "source": [
    "# import scalers\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kIv-Bld02iv",
    "outputId": "a5a131a9-a5a3-4aac-e4dc-e83b2954c29c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.8157248157248157)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Basic Decision Tree Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# model instace\n",
    "dt_model = DecisionTreeClassifier(random_state=10)\n",
    "\n",
    "# fitting the model\n",
    "dt_model.fit(train_x, train_y)\n",
    "\n",
    "# make training prediction\n",
    "train_yhat = dt_model.predict(train_x)\n",
    "train_score = recall_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = dt_model.predict(test_x)\n",
    "test_score = recall_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eeIPUlN02iw"
   },
   "outputs": [],
   "source": [
    "## Tuning depth of the tree\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for depth in range(1,20):\n",
    "    dt_model = DecisionTreeClassifier(max_depth=depth, random_state=10)\n",
    "    dt_model.fit(train_x, train_y)\n",
    "    train_yhat = dt_model.predict(train_x)\n",
    "    train_score.append(recall_score(train_y, train_yhat))\n",
    "    test_yhat = dt_model.predict(test_x)\n",
    "    test_score.append(recall_score(test_y, test_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Di73gRjO02ix"
   },
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({'max_depth':range(1,20), 'train_score':train_score, 'test_score':test_score})\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAw-IbXP02ix"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['max_depth'], frame['train_score'], marker='o')\n",
    "plt.plot(frame['max_depth'], frame['test_score'], marker='o')\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbak4dzD02ix"
   },
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(max_depth=11, random_state=10, class_weight = 'balanced')\n",
    "# fitting the model\n",
    "dt_model.fit(train_x, train_y)\n",
    "\n",
    "# make training prediction\n",
    "train_yhat = dt_model.predict(train_x)\n",
    "train_score = recall_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = dt_model.predict(test_x)\n",
    "test_score = recall_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI7KI3UL02iy",
    "outputId": "49fb47f7-a258-48d2-acea-0a0433197ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=DecisionTreeClassifier(class_weight='balanced',\n",
       "                                                    random_state=10),\n",
       "                   n_iter=500,\n",
       "                   param_distributions={'ccp_alpha': [0, 0.001, 0.01, 0.1, 1],\n",
       "                                        'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [5, 10, 11, 15],\n",
       "                                        'max_leaf_nodes': [2, 7, 12, 17, 22, 27,\n",
       "                                                           32, 37, 42, 47, 52,\n",
       "                                                           57, 62, 67, 72, 77,\n",
       "                                                           82, 87, 92, 97],\n",
       "                                        'min_samples_split': [2, 7, 12, 17, 22,\n",
       "                                                              27, 32, 37, 42,\n",
       "                                                              47, 52, 57, 62,\n",
       "                                                              67, 72, 77, 82,\n",
       "                                                              87, 92, 97]},\n",
       "                   scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Gridsearch CV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "params = {'max_depth': [5, 10, 11, 15], 'max_leaf_nodes': list(range(2, 100, 5)), 'min_samples_split': list(range(2, 100, 5)), 'criterion': ['gini', 'entropy'], 'ccp_alpha': [0, 0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "\n",
    "grid_search_cv = RandomizedSearchCV(DecisionTreeClassifier(random_state=10, class_weight = 'balanced'), params, n_iter = 500, verbose=1, cv=3, scoring = 'f1')\n",
    "grid_search_cv.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKljlT5p02iy",
    "outputId": "f9ed1b57-c1cb-4eb1-b754-b4716ca764b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.001, class_weight='balanced',\n",
       "                       criterion='entropy', max_depth=10, max_leaf_nodes=77,\n",
       "                       min_samples_split=7, random_state=10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0PEVjS402iy",
    "outputId": "d16a08a1-87c5-4805-c099-03e3cade15ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8587937883712532, 0.7904967602591793)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model = DecisionTreeClassifier(ccp_alpha=0.001, class_weight='balanced',\n",
    "                       criterion='entropy', max_depth=10, max_leaf_nodes=77,\n",
    "                       min_samples_split=7, random_state=10)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# fitting the model\n",
    "dt_model.fit(train_x, train_y)\n",
    "\n",
    "# make training prediction\n",
    "train_yhat = dt_model.predict(train_x)\n",
    "train_score = f1_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = dt_model.predict(test_x)\n",
    "test_score = f1_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbWyscLM02iz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dt_model = DecisionTreeClassifier(ccp_alpha=1, class_weight='balanced',\n",
    "                       criterion='entropy', max_depth=10, random_state=10)\n",
    "\n",
    "cross_val_score(dt_model, train_x, train_y, cv=3, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9fEJi2502i0"
   },
   "source": [
    "### Try the whole procedure of tuning with `class_weight = 'balanced'`. Do you observe any improvement in the recall score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DTnjZVT02i1"
   },
   "source": [
    "### Try the whole procedure of tuning without scaling the variables or with a MinMaxScaler(). Do you observe any improvement in the recall score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9UKn7XW02i1"
   },
   "source": [
    "### Try the whole procedure of tuning with `RandomizedSearchCV()`. Does it reduce the computation time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tpc3anBN02i2"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "rf_model = RF(random_state=10, class_weight = 'balanced')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# fitting the model\n",
    "rf_model.fit(train_x, train_y)\n",
    "\n",
    "# make training prediction\n",
    "train_yhat = rf_model.predict(train_x)\n",
    "train_score = recall_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = rf_model.predict(test_x)\n",
    "test_score = recall_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxztsQxw02i2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "params = {'max_depth': [5, 10, 11, 15], 'max_leaf_nodes': list(range(2, 100, 5)), 'min_samples_split': list(range(2, 100, 5)), 'criterion': ['gini', 'entropy'], 'ccp_alpha': [0, 0.001, 0.01, 0.1, 1], 'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', 'log2']}\n",
    "\n",
    "\n",
    "grid_search_cv = RandomizedSearchCV(RF(random_state=10, class_weight = 'balanced'), params, n_iter = 600, verbose=1, cv=3, scoring = 'f1')\n",
    "grid_search_cv.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSJYCxyp02i2"
   },
   "outputs": [],
   "source": [
    "grid_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlIfrZtM02i3",
    "outputId": "fc83eb86-e6b9-40cd-9984-1f45a490752d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8962655601659751, 0.8398169336384439)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "rf_model = RF(ccp_alpha=0, class_weight='balanced',\n",
    "                       criterion='entropy', max_depth=10, max_leaf_nodes=97,\n",
    "                       min_samples_split=22, n_estimators=500, random_state=10)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# fitting the model\n",
    "rf_model.fit(train_x, train_y)\n",
    "\n",
    "# make training prediction\n",
    "train_yhat = rf_model.predict(train_x)\n",
    "train_score = f1_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = rf_model.predict(test_x)\n",
    "test_score = f1_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxPXeVD_02i3",
    "outputId": "644d17ee-6420-423a-fad2-2c2576e19cb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:51:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9168765743073047)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "# make training prediction\n",
    "train_yhat = model.predict(train_x)\n",
    "train_score = f1_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = model.predict(test_x)\n",
    "test_score = f1_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEYeYw5O02i4",
    "outputId": "1b54ec79-c479-40e4-b028-be69a280e27e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaushik\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:38:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.3, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0.01, reg_lambda=10, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_dist = {\"max_depth\": [5, 10, 15, 30, 50],\n",
    "              \"n_estimators\": [100, 200, 300],\n",
    "              \"learning_rate\": [0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "             \"reg_alpha\": [0, 0.01, 0.1, 1, 10],\n",
    "             \"reg_lambda\": [0, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid_search = RandomizedSearchCV(model, param_dist, cv = 3, n_iter = 50,  \n",
    "                                   verbose=10, n_jobs=-1, scoring = \"f1\")\n",
    "\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jspWhHKu02i4",
    "outputId": "dbc22caa-2485-4cd0-e1e2-80e55f8d8cfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaushik\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:43:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9995903318312167, 0.9209535759096613)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "model = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.3, max_delta_step=0, max_depth=5,\n",
    "              min_child_weight=1, monotone_constraints='()',\n",
    "              n_estimators=200, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
    "              reg_alpha=0.01, reg_lambda=10, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "# make training prediction\n",
    "train_yhat = model.predict(train_x)\n",
    "train_score = f1_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = model.predict(test_x)\n",
    "test_score = f1_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2PUxn6t02i4",
    "outputId": "23d69b9e-9219-4327-8117-258bf3f394c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting LogitBoost\n",
      "  Downloading logitboost-0.7-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kaushik\\anaconda3\\lib\\site-packages (from LogitBoost) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\kaushik\\anaconda3\\lib\\site-packages (from LogitBoost) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kaushik\\anaconda3\\lib\\site-packages (from LogitBoost) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kaushik\\anaconda3\\lib\\site-packages (from scikit-learn->LogitBoost) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\kaushik\\anaconda3\\lib\\site-packages (from scikit-learn->LogitBoost) (0.17.0)\n",
      "Installing collected packages: LogitBoost\n",
      "Successfully installed LogitBoost-0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install LogitBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfLhggJA02i5",
    "outputId": "77ae2925-88f0-4871-9e46-805dc3410414"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9170159262363788, 0.8972431077694234)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from logitboost import LogitBoost\n",
    "\n",
    "lboost = LogitBoost(n_estimators=200, random_state=0)\n",
    "lboost.fit(train_x, train_y)\n",
    "\n",
    "# make training prediction\n",
    "train_yhat = lboost.predict(train_x)\n",
    "train_score = f1_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = lboost.predict(test_x)\n",
    "test_score = f1_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-nRJngN02i5",
    "outputId": "aa35506c-80df-44ea-ad55-fb6327b37939"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaushik\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9869706840390878, 0.8986568986568987)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=500, random_state=10)),\n",
    "    ('lr', LogitBoost(n_estimators=200, random_state=0))\n",
    "]\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "              importance_type='gain', interaction_constraints='',\n",
    "              learning_rate=0.3, max_delta_step=0, max_depth=5,\n",
    "              min_child_weight=1, monotone_constraints='()',\n",
    "              n_estimators=200, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
    "              reg_alpha=0.01, reg_lambda=10, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    " )\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# make training prediction\n",
    "train_yhat = clf.predict(train_x)\n",
    "train_score = f1_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = clf.predict(test_x)\n",
    "test_score = f1_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brkscfVU02i6",
    "outputId": "6783fb81-40b8-4ae6-ecec-1c7316613f3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7778819119025304, 0.7033285094066569)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(train_x, train_y)\n",
    "\n",
    "# make training prediction\n",
    "train_yhat = svc_model.predict(train_x)\n",
    "train_score = f1_score(train_y, train_yhat)\n",
    "\n",
    "# make test prediction\n",
    "test_yhat = svc_model.predict(test_x)\n",
    "test_score = f1_score(test_y, test_yhat)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpyTse4B02i6",
    "outputId": "9f7b1a25-6cc7-444d-c4ac-13bbe9d807d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\"C\":[0.01, 0.1, 1, 10, 100, 1000, 10000],\n",
    "             \"kernel\": ['linear', 'poly', 'rbf'],\n",
    "             \"degree\": [2, 3, 4],\n",
    "             \"gamma\": ['auto', 'scale']}\n",
    "\n",
    "grid_search = RandomizedSearchCV(svc_model, param_dist, cv = 3, n_iter = 30,  \n",
    "                                   verbose=10, n_jobs=-1, scoring = \"f1\")\n",
    "\n",
    "grid_search.fit(train_x, train_y)\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day 4 - Tree - based models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
