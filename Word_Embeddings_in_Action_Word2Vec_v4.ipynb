{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWWH8Gvgd09_"
   },
   "source": [
    "# Word Embeddings in Action - Word2Vec\n",
    "\n",
    "### Word embeddings are a really useful way of converting text into a format that is interpretable to the model while still keeping it's semantic meaning intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gPnpxm2s7Zty"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "AgII8L1Xd0-Q",
    "outputId": "6e3f6605-bfee-4605-d913-cdf30c5db1f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      1   \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone  \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/  \n",
       "2          We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu  \n",
       "3                     I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/  \n",
       "4         What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the twitter dataset\n",
    "df = pd.read_csv('tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epUh25Opuw3H"
   },
   "source": [
    "Skip the code block below if you have already downloaded the stopwords before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FtVATpXrmVy2",
    "outputId": "f3600cd6-2c41-40b9-fd57-6a169ab48551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/analyticsvidhya/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "AuuF55_xnWYY",
    "outputId": "62b3962f-9827-410d-cc5a-70a787b8cd12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yy-hraLZnJYC"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pENuV1ltlJeM"
   },
   "outputs": [],
   "source": [
    "# function for text pre-processing\n",
    "def tweet_cleaner(text):\n",
    "    newString=re.sub(r'@[A-Za-z0-9]+','',text)                     #removing user mentions\n",
    "    newString=re.sub(\"#\",\"\",newString)                             #removing hashtag symbol\n",
    "    newString= re.sub(r'http\\S+', '', newString)                   #removing links\n",
    "    newString= re.sub(r\"'s\\b\",\"\",newString)                        #removing 's\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", newString)             #Fetching out only letters\n",
    "    lower_case = letters_only.lower()                              #converting everything to lowercase\n",
    "    tokens = [w for w in lower_case.split() if not w in stop_words]#stopwords removal\n",
    "    newString=''\n",
    "    for i in tokens:                                                 \n",
    "        newString=newString+lemmatizer.lemmatize(i)+' '            #converting words to lemma                               \n",
    "    \n",
    "    return newString.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFPeit-wlQ83"
   },
   "outputs": [],
   "source": [
    "# empty list to store tweets after pre-processing\n",
    "cleaned_tweets = []\n",
    "\n",
    "# pre-processing the tweets\n",
    "for i in df['tweet']:\n",
    "  cleaned_tweets.append(tweet_cleaner(i))\n",
    "\n",
    "#creating new column  \n",
    "df['cleaned_tweets']= cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "nG9HS92gnjhQ",
    "outputId": "7bc3b556-3961-4c48-faab-9cd51013710e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "      <td>fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "      <td>finally transparant silicon case thanks uncle yay sony xperia sonyexperias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "      <td>love would go talk makememories unplug relax iphone smartphone wifi connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "      <td>wired know george made way iphone cute daventry home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "      <td>amazing service apple even talk question unless pay stupid support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      1   \n",
       "\n",
       "                                                                                                                                 tweet  \\\n",
       "0     #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone   \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/   \n",
       "2          We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu   \n",
       "3                     I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/   \n",
       "4         What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!   \n",
       "\n",
       "                                                                                    cleaned_tweets  \n",
       "0  fingerprint pregnancy test android apps beautiful cute health igers iphoneonly iphonesia iphone  \n",
       "1                       finally transparant silicon case thanks uncle yay sony xperia sonyexperias  \n",
       "2                      love would go talk makememories unplug relax iphone smartphone wifi connect  \n",
       "3                                             wired know george made way iphone cute daventry home  \n",
       "4                               amazing service apple even talk question unless pay stupid support  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Fpt6-ned1AI"
   },
   "source": [
    "### Using Google's pre-trained Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "WQNZ7Exh3jky",
    "outputId": "d22cb1b5-0d68-46a9-d079-c52e84e1a0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-17 05:20:00--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.18.43\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.18.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  67.8MB/s    in 23s     \n",
      "\n",
      "2019-06-17 05:20:23 (68.8 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download and extract word2vec embeddings \n",
    "! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "! gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "GoHGUSuId1AO",
    "outputId": "1b0d7a89-ad6c-4d21-ffc7-e645a0e3614e"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# path of the downloaded model\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# load into gensim\n",
    "w2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5WyFsVHd1Aj"
   },
   "source": [
    "Once you have executed the above code, your word2vec model is finally installed and loaded in gensim. Let's explore some of the features of this model.\n",
    "\n",
    "__Contextual Relationship Between Words__\n",
    "\n",
    " - One of the impressive things about word2vec is it's ability to capture semantic relationship between words. That is the reason that you can do cool stuff like perform linear algebra on words and get an appropriate output. Have a look at the following example:\n",
    "\n",
    "    `airplane - fly + drive = car`\n",
    "\n",
    " - If you pass the left hand side of the above equation to the model, it will give the right handside. Which makes sense because what would you get if you remove the ability to fly from an airplane? And add the ability to drive? You would get a car!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "assnsO6Ld1Ap"
   },
   "source": [
    "### Text Classification using Word2Vec\n",
    "\n",
    "Let's now get back to our task to classify our twitter data by using __word2vec__ as features. However, word2vec gives vector representation of individual words, in order to find the same for a statement or a document you can take mean of the vectors of it's constituent words.\n",
    "\n",
    "<br>\n",
    "\n",
    "This is what we will do in the *get_embedding_w2v()* function. It will iterate through all the terms in a tweet and extract a fixed size vector for each of the terms given that they are present in the vocabulary of the word2vec model.\n",
    "\n",
    "<br>\n",
    "\n",
    "Please note that the length of every vector of the pre-trained word2vec embeddings is 300.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGTv0F10qdhO"
   },
   "outputs": [],
   "source": [
    "# function to get vector representation of a tweet\n",
    "def word_vector(tokens):\n",
    "    vec = np.zeros((1,300))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2vec.wv.word_vec(word)\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xV0yy-P0sPKf",
    "outputId": "a050f111-1041-463d-e379-e3e74281e408"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/analyticsvidhya/miniconda3/envs/datahack/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# empty array of shape (no. of tweets X 300) to store word2vec features\n",
    "wordvec_arrays = np.zeros((len(df), 300))\n",
    "\n",
    "for i,j in enumerate(df['cleaned_tweets']):\n",
    "  wordvec_arrays[i,:] = word_vector(j.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lebArT-0tIXg",
    "outputId": "6aa90c38-3b4d-45b0-fc8e-b6c4d8df6db9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t037dps1d1BL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train and test\n",
    "y = df['label']\n",
    "X_train_wv, X_test_wv, y_train_wv, y_test_wv = train_test_split(wordvec_arrays, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLKwHmgtd1Bl"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "Tn6YmHjBd1B4",
    "outputId": "a5bc31ce-fab5-48e0-ff82-c76b627d30f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/analyticsvidhya/miniconda3/envs/datahack/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model on the train dataset\n",
    "model = model.fit(X_train_wv, y_train_wv)\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "pred = model.predict(X_test_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1SkCDYjWd1CI",
    "outputId": "f055af63-88d6-4345-f0ff-c22a39519a36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7600487210718635\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy of the model\n",
    "print(\"F1 Score:\", f1_score(y_test_wv, pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word Embeddings in Action - Word2Vec_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
