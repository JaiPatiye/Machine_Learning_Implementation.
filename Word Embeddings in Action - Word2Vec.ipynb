{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings in Action - Word2Vec\n",
    "\n",
    " - Word embeddings are a really useful way of converting text into a format that is interpretable to the model while still keeping it's semantic meaning intact.\n",
    " - Now that you have already learnt the theory behind Word2Vec, in this notebook you will learn how to utilize it practically on a real-world data set.\n",
    " \n",
    "![](https://www.tensorflow.org/images/linear-relationships.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cairn shares slump on oil setback\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Egypt to sell off state-owned bank\\n\\nThe Egyp...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cairn shares up on new oil find\\n\\nShares in C...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Low-cost airlines hit Eurotunnel\\n\\nChannel Tu...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parmalat to return to stockmarket\\n\\nParmalat,...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  Cairn shares slump on oil setback\\n\\nShares in...  business\n",
       "1  Egypt to sell off state-owned bank\\n\\nThe Egyp...  business\n",
       "2  Cairn shares up on new oil find\\n\\nShares in C...  business\n",
       "3  Low-cost airlines hit Eurotunnel\\n\\nChannel Tu...  business\n",
       "4  Parmalat to return to stockmarket\\n\\nParmalat,...  business"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "bbc_news = pd.read_csv('../datasets/bbc_news_mixed.csv')\n",
    "bbc_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that you are going to use is a collection of news articles from BBC across 5 major categories, namely:\n",
    " \n",
    " - Business\n",
    " - Entertainment\n",
    " - Politics\n",
    " - Sport\n",
    " - Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cairn shares slump on oil setback\n",
      "\n",
      "Shares in Cairn Energy, a UK oil firm, have closed down 18% after a disappointing drilling update and a warning over possible tax demands.\n",
      "\n",
      "The company said tests ha\n",
      "Egypt to sell off state-owned bank\n",
      "\n",
      "The Egyptian government is reportedly planning to privatise one of the country's big public banks.\n",
      "\n",
      "An Investment Ministry official has told the Reuters news agency\n"
     ]
    }
   ],
   "source": [
    "# print first 3 articles\n",
    "for art in bbc_news.text[:3]:\n",
    "    print(art[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an idea of how your data looks like, let's see the count of each category in the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category-wise count\n",
    "bbc_news.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Google's pre-trained Word2Vec\n",
    "\n",
    "Google has released a pre-trained Word2Vec model that has the advantage of being trained on **Google's News data set of 3 million words**. You can __download__ the word2vec embeddings from this [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).\n",
    "\n",
    "__Installation__\n",
    "\n",
    " - Make sure you have downloaded it in the same folder where this Jupyter notebook is residing.\n",
    " \n",
    " - Once you have finished downloading, you need to decompress the file by the following command in your terminal.\n",
    " \n",
    " `gzip -d GoogleNews-vectors-negative300.bin.gz`\n",
    " - Now that you have the model downloaded and ready, let's see how to import it in Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# path of the downloaded model\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "# load into gensim\n",
    "w2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have executed the above code, your word2vec model is finally installed and loaded in gensim. Let's explore some of the features of this model.\n",
    "\n",
    "__Contextual Relationship Between Words__\n",
    "\n",
    " - One of the impressive things about word2vec is it's ability to capture semantic relationship between words. That is the reason that you can do cool stuff like perform linear algebra on words and get an appropriate output. Have a look at the following example:\n",
    "\n",
    "    `airplane - fly + drive = car`\n",
    "\n",
    " - If you pass the left hand side of the above equation to the model, it will give the right handside. Which makes sense because what would you get if you remove the ability to fly from an airplane? And add the ability to drive? You would get a car!\n",
    " - The underlying model is able to understand implicit relationship between airplane and fly and also how removing the medium of travel changes the machine used to travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 0.511200487613678),\n",
       " ('drives', 0.47777241468429565),\n",
       " ('automobile', 0.45616623759269714),\n",
       " ('vehicle', 0.44856154918670654),\n",
       " ('SUV', 0.44360122084617615)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# airplane - fly + drive\n",
    "w2vec.most_similar(positive=['airplane', 'drive'], negative=['fly'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few other examples\n",
    "\n",
    "1. `king - man + woman = queen`\n",
    "Removing man from kind and adding woman gives queen.\n",
    "2. `moscow - russia + japan = tokyo`\n",
    "Removing russia from moscow(capital of russia) and adding japan gives tokyo(capital of japan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431607246399), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133)]\n",
      "[('tokyo', 0.49488699436187744), ('hawaii', 0.4802300035953522), ('malta', 0.4749153256416321), ('japanese', 0.45723214745521545), ('seattle', 0.4502826929092407)]\n"
     ]
    }
   ],
   "source": [
    "# king - man + woman\n",
    "print(w2vec.most_similar(positive=['king', 'woman'], negative=['man'], topn=5))\n",
    "# moscow - russia + japan\n",
    "print(w2vec.most_similar(positive=['moscow', 'japan'], negative=['russia'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification using Word2Vec\n",
    "\n",
    "Let's now get back to our BBC News data set. We will classify the data into different categories by using Word2Vec as a text feature. However, Word2Vec gives vector representation of individual words, in order to find the same for a statement or a document you can take mean of the vectors of it's constituent words.\n",
    "\n",
    "This is what we will do in the `get_embedding_w2v()`. It iterates through all the words in a document/statement and extracts the vector for them if they are present in the vocabulary of the word2vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns vector reperesentation\n",
    "def get_embedding_w2v(doc_tokens, pre_trained):\n",
    "    for tok in doc_tokens:\n",
    "        if tok in w2vec.wv.vocab:\n",
    "            embeddings.append(w2vec.wv.word_vec(tok))\n",
    "    # mean the vectors of individual words to get the vector of the entire text\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now we will follow the steps below:\n",
    "\n",
    " - Generate vector representation for each document. Note that here `pre_trained=1` should be used.\n",
    " - Create the X data set and split it into train and test sets.\n",
    " - Train a text classification (Naive Bayes) model and compute it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2225, 300)\n"
     ]
    }
   ],
   "source": [
    "# general preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create X from w2vec\n",
    "X_w2v = preprocessed_bbc.apply(lambda x: get_embedding_w2v(x))\n",
    "X_w2v = pd.DataFrame(X_w2v.tolist())\n",
    "print('X shape:', X_w2v.shape)\n",
    "\n",
    "# split into train and test\n",
    "y = bbc_news.label\n",
    "X_train_wv, X_test_wv, y_train_wv, y_test_wv = train_test_split(X_w2v, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.80898876404494 %\n"
     ]
    }
   ],
   "source": [
    "# build a text classification model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize GaussianNB classifier\n",
    "model = GaussianNB()\n",
    "# Fit the model on the train dataset\n",
    "model = model.fit(X_train_wv, y_train_wv)\n",
    "# Make predictions on the test dataset\n",
    "pred = model.predict(X_test_wv)\n",
    "\n",
    "# check the accuracy of the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test_wv, pred)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is pretty impressive by using the Google's pre-trained model (92.8 %). This is because these word embeddings have been trained on a rich vocabulary of 3 million words. The more rich the vocabulary is, the better the model generates the semantic vectors of a word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
